<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.4.3">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2018-01-23T21:28:36-05:00</updated><id>/</id><title type="html">Data Points</title><subtitle>Once upon a time, when processor clock speeds were in the kHz, memory modules were woven by hand and storage was on tape, it made sense to use the minimum feasible representations of our data. Nowadays ? Not so much.   </subtitle><entry><title type="html">UPDATE Considered Harmful</title><link href="/metadata/2018/01/23/UPDATE-Considered-Harmful.html" rel="alternate" type="text/html" title="UPDATE Considered Harmful" /><published>2018-01-23T00:00:00-05:00</published><updated>2018-01-23T00:00:00-05:00</updated><id>/metadata/2018/01/23/UPDATE-Considered-Harmful</id><content type="html" xml:base="/metadata/2018/01/23/UPDATE-Considered-Harmful.html">&lt;p&gt;In 1968, Prof. Dijkstra submitted a letter to the “ Communications of the ACM “ which he titled “ A case against the GOTO statement “. A subeditor - clearly destined for greater things - rebranded it “ GOTO Considered Harmful “ and that phrase, quoted and misquoted many times over nearly half a century, has been with us ever since.&lt;/p&gt;

&lt;p&gt;It was born in a time when you were expected to prove rigorously that your code would work correctly under all conditions - my initiation to that world was to design and implement a long-division procedure for hardware which supported only integer division. Prof. Dijkstra’s insight was that once you had introduced GOTOs and their corresponding targets into your code, it became exponentially harder to determine the path that program execution might have taken in order to arrive at one of those targets: to determine the history by which a particular program state had been reached.&lt;/p&gt;

&lt;p&gt;For most of the intervening half-century database servers have offered us a destructive UPDATE command - to allow us to specify a new value in a database to replace an old one. This, of course, poses exactly the same problem. It does not support the ability to ask the database server how the state of a particular record evolved over time.&lt;/p&gt;

&lt;p&gt;Following their tradition of shipping minimally acceptable products, the database vendors allowed us to see the commands that had operated on a record by interpreting the transaction log - so long as we hadn’t truncated it to save space and keep the database running. Eventually, facilities like SQL Server’s CDC emerged that simplify things greatly - but even so, provide none of the metadata surrounding an update that would be needed for the database to properly distinguish, for example, whether a new value was truly a new value, a revision, or a correction for a mistaken observation of an unchanged value.&lt;/p&gt;

&lt;p&gt;The metadata for database changes should reside in the database itself - otherwise it will suffer from the same kind of digital rot as documentation. Even better, the history of a value in a record - metadata and all - should live in the same table as the current value. Rather than holding just an “ int “ value for each record, the table should hold a stack of “ int “ values and their metadata - when was the value entered, by whom, why and so forth.&lt;/p&gt;

&lt;p&gt;Why is this important ?&lt;/p&gt;

&lt;p&gt;Because Disruption.&lt;/p&gt;

&lt;p&gt;In a competitive business environment, you as a “ data guy “ have no idea where the next challenge to your company’s existing business is going to come from, or the one after that, or the one after that… In that case you had better make sure that you are keeping every scrap of information that you can.&lt;/p&gt;

&lt;p&gt;Otherwise, you’re not working with a database at all, but an application-specific data store - it meets the business’ current needs, but at any moment, with a day or less notice, you’ll have to tear it up, start over and spend a huge amount of effort reconstituting data from spreadsheets or worse. And, it could be argued, you are not fulfilling your fiduciary duty to your employers and their clients.&lt;/p&gt;

&lt;p&gt;My thoughts on the DELETE command are left as an exercise for the reader…&lt;/p&gt;</content><author><name></name></author><summary type="html">In 1968, Prof. Dijkstra submitted a letter to the “ Communications of the ACM “ which he titled “ A case against the GOTO statement “. A subeditor - clearly destined for greater things - rebranded it “ GOTO Considered Harmful “ and that phrase, quoted and misquoted many times over nearly half a century, has been with us ever since. It was born in a time when you were expected to prove rigorously that your code would work correctly under all conditions - my initiation to that world was to design and implement a long-division procedure for hardware which supported only integer division. Prof. Dijkstra’s insight was that once you had introduced GOTOs and their corresponding targets into your code, it became exponentially harder to determine the path that program execution might have taken in order to arrive at one of those targets: to determine the history by which a particular program state had been reached. For most of the intervening half-century database servers have offered us a destructive UPDATE command - to allow us to specify a new value in a database to replace an old one. This, of course, poses exactly the same problem. It does not support the ability to ask the database server how the state of a particular record evolved over time. Following their tradition of shipping minimally acceptable products, the database vendors allowed us to see the commands that had operated on a record by interpreting the transaction log - so long as we hadn’t truncated it to save space and keep the database running. Eventually, facilities like SQL Server’s CDC emerged that simplify things greatly - but even so, provide none of the metadata surrounding an update that would be needed for the database to properly distinguish, for example, whether a new value was truly a new value, a revision, or a correction for a mistaken observation of an unchanged value. The metadata for database changes should reside in the database itself - otherwise it will suffer from the same kind of digital rot as documentation. Even better, the history of a value in a record - metadata and all - should live in the same table as the current value. Rather than holding just an “ int “ value for each record, the table should hold a stack of “ int “ values and their metadata - when was the value entered, by whom, why and so forth. Why is this important ? Because Disruption. In a competitive business environment, you as a “ data guy “ have no idea where the next challenge to your company’s existing business is going to come from, or the one after that, or the one after that… In that case you had better make sure that you are keeping every scrap of information that you can. Otherwise, you’re not working with a database at all, but an application-specific data store - it meets the business’ current needs, but at any moment, with a day or less notice, you’ll have to tear it up, start over and spend a huge amount of effort reconstituting data from spreadsheets or worse. And, it could be argued, you are not fulfilling your fiduciary duty to your employers and their clients. My thoughts on the DELETE command are left as an exercise for the reader…</summary></entry><entry><title type="html">Five 9s</title><link href="/errors/2018/01/14/Five-9s.html" rel="alternate" type="text/html" title="Five 9s" /><published>2018-01-14T00:00:00-05:00</published><updated>2018-01-14T00:00:00-05:00</updated><id>/errors/2018/01/14/Five-9s</id><content type="html" xml:base="/errors/2018/01/14/Five-9s.html">&lt;p&gt;Back in the days when dinosaurs - or at least 18-wheelers with “ Prime Computer “ and “ Digital “ painted on their sides - roamed Route 128 and the Mass Pike, there were attempts to create reliable systems from unreliable components. But minicomputers were too expensive to survive when the PC revolution arrived, and fault-tolerant minicomputers were REALLY expensive.&lt;/p&gt;

&lt;p&gt;Whatever approach the system architects adopted, the idea was to deliver something like 99.999% reliability. Which sounds impressive until you realize that this allows five minutes failure in a year - and if those five minutes coincided with the first five minutes of equity trading on an important day, then the fur would fly regardless.&lt;/p&gt;

&lt;p&gt;Fast forward a few decades.&lt;/p&gt;

&lt;p&gt;Hopefully your Data Governance Committee/Board will accept that a quality target of 100% accuracy is out of reach. Offer them a target of 99.999% and the meeting will probably move on happily to the next agenda item.&lt;/p&gt;

&lt;p&gt;In our world of Big Data that is just not good enough - if your processing uses 100,000,000 data points a day, 99.999% accuracy means 1,000 errors. Even with your Data Quality team working at full speed 24 hours a day, they would need to find and fix one error every minute to clean them up.&lt;/p&gt;

&lt;p&gt;And while your annual goals might well include a quality target, when things go wrong that isn’t how you’ll be judged. What will actually count is the effect of an error on the overall result : perhaps a cash flow item with the wrong sign in a quarterly financial report’s data causes you to buy into the shares of a company that declares bankruptcy later that same day. But if the error didn’t change your day’s trading, the existence of that error wouldn’t have mattered even if it had been noticed.&lt;/p&gt;

&lt;p&gt;Catching those problems ahead of time is almost impossible unless you can afford a Space Shuttle-like investment in multiple implementations of the same system. Otherwise, you will have to attack the problem from two directions at the same time  :&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Understand the data well enough so that you can construct heuristics to find suspicious values in the raw data before it’s used ( for a company that files its results every quarter, that cash flow value should be unchanged for 89 days in a row, so a change at any other time is automatically suspicious ), and&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Understand your process well enough so that you can construct heuristics to recognize “ unlikely “ data values after they are calculated but before they are used .&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Whatever happens, data errors will always be with you. And the post-mortems will always include the question “ why didn’t you catch it in time ? “&lt;/p&gt;</content><author><name></name></author><summary type="html">Back in the days when dinosaurs - or at least 18-wheelers with “ Prime Computer “ and “ Digital “ painted on their sides - roamed Route 128 and the Mass Pike, there were attempts to create reliable systems from unreliable components. But minicomputers were too expensive to survive when the PC revolution arrived, and fault-tolerant minicomputers were REALLY expensive. Whatever approach the system architects adopted, the idea was to deliver something like 99.999% reliability. Which sounds impressive until you realize that this allows five minutes failure in a year - and if those five minutes coincided with the first five minutes of equity trading on an important day, then the fur would fly regardless. Fast forward a few decades. Hopefully your Data Governance Committee/Board will accept that a quality target of 100% accuracy is out of reach. Offer them a target of 99.999% and the meeting will probably move on happily to the next agenda item. In our world of Big Data that is just not good enough - if your processing uses 100,000,000 data points a day, 99.999% accuracy means 1,000 errors. Even with your Data Quality team working at full speed 24 hours a day, they would need to find and fix one error every minute to clean them up. And while your annual goals might well include a quality target, when things go wrong that isn’t how you’ll be judged. What will actually count is the effect of an error on the overall result : perhaps a cash flow item with the wrong sign in a quarterly financial report’s data causes you to buy into the shares of a company that declares bankruptcy later that same day. But if the error didn’t change your day’s trading, the existence of that error wouldn’t have mattered even if it had been noticed. Catching those problems ahead of time is almost impossible unless you can afford a Space Shuttle-like investment in multiple implementations of the same system. Otherwise, you will have to attack the problem from two directions at the same time : Understand the data well enough so that you can construct heuristics to find suspicious values in the raw data before it’s used ( for a company that files its results every quarter, that cash flow value should be unchanged for 89 days in a row, so a change at any other time is automatically suspicious ), and Understand your process well enough so that you can construct heuristics to recognize “ unlikely “ data values after they are calculated but before they are used . Whatever happens, data errors will always be with you. And the post-mortems will always include the question “ why didn’t you catch it in time ? “</summary></entry><entry><title type="html">Everything is a Timeseries</title><link href="/timeseries/2018/01/11/everything-is-a-timeseries.html" rel="alternate" type="text/html" title="Everything is a Timeseries" /><published>2018-01-11T00:00:00-05:00</published><updated>2018-01-11T00:00:00-05:00</updated><id>/timeseries/2018/01/11/everything-is-a-timeseries</id><content type="html" xml:base="/timeseries/2018/01/11/everything-is-a-timeseries.html">&lt;p&gt;The year : 1987&lt;/p&gt;

&lt;p&gt;The location : a Strangelove-ian conference room ( although with lots of BIG windows ) overlooking Sixth Avenue&lt;/p&gt;

&lt;p&gt;The players : me and the Great Man. He is beyond upset : veins are throbbing in both temples and, instead of just being red, his face is blotched with white.&lt;/p&gt;

&lt;p&gt;The takeaway : ( paraphrased for those of a delicate nature ) Everything is a Time-series&lt;/p&gt;

&lt;p&gt;The time-series schema he was thinking of went something like :&lt;/p&gt;

&lt;p&gt;&lt;em&gt;A discrete time series is a set of time-ordered data {X&lt;sub&gt;t1&lt;/sub&gt;, X&lt;sub&gt;t2&lt;/sub&gt;, …,X&lt;sub&gt;tn&lt;/sub&gt;} obtained from observations of some phenomenon over time. Throughout this documentation we will assume, as is commonly done, that the observations are made at equally spaced time intervals. This assumption enables us to use the interval between two successive observations as the unit of time and, without any loss of generality, we will denote the time series by {X&lt;sub&gt;1&lt;/sub&gt;, X&lt;sub&gt;2&lt;/sub&gt;,…X&lt;sub&gt;n&lt;/sub&gt;}.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Which, depressingly, was taken from a current document, rather than some well-worn, quarter-century old, yellowed papers at the back of one of my desk drawers. Why “ depressingly “ ?&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;From www.bea.gov - discussing GDP figures for the US economy, surely one of the most widely used of all publicly-available data values :&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;The Bureau emphasized that the first-quarter advance estimate released today is based on source data that are incomplete or subject to further revision by the source agency (see the box on page 3 and “Comparisons of Revisions to GDP” on page 5). The “second” estimate for the first quarter, based on more complete data, will be released on May 29, 2015.&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;And as a special treat :&lt;/p&gt;

    &lt;p&gt;&lt;em&gt;The annual revision of the national income and product accounts will be released along with the “advance” estimate of GDP for the second quarter of 2015 on July 30. In addition to the regular revision of the estimates for the most recent 3 years and for the first quarter of 2015, some series will be revised back further (for more information, see the Technical Note).&lt;/em&gt;&lt;/p&gt;

    &lt;p&gt;So in this case, not only will the initially published value for 1Q2015 be changed using the current methodology, but it, along with twelve other values, will shortly be recalculated using a new methodology.&lt;/p&gt;

    &lt;p&gt;How would your database track these changing values for 1Q2015 ? As exceptions - when each revision as it comes in provokes a fire drill ? Or as a time-series ? - since the X&lt;sub&gt;t&lt;/sub&gt; values are already a time-series, it would need a second time dimension, but that would violate the naïve definition of a time-series and lead us into the land of bi-temporal data.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In that quarter-century, relational databases have become the &lt;em&gt;de facto&lt;/em&gt; standard technology for storing data of all kinds - because the underlying logic is so very flexible that they can, in principle, store and retrieve anything.&lt;/p&gt;

    &lt;p&gt;Unfortunately, the original implementations were very crude ( Sitting on one of my bookshelves is a 113 page book from 1978, that includes the complete source code - in RT11 Basic - for a relational database system ) . And later versions went through the usual hype-cycle driven changes rather than addressing real-world problems.&lt;/p&gt;

    &lt;p&gt;As a result, the number of shops where data with intrinsic ordering ( like our naïve time-series definition ) can be managed natively, is small. There were brave attempts to “ bolt-on “ time-series handling to mainstream databases in the 1990s, but in my career, I never met one in production use. Guess how much rarer a database that can handle two time dimensions natively going to be ?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Most depressing of all, in the job interviews I took part in over that time, no CS graduate had the faintest idea about how to tackle these problems. Sure, they knew how to build a CRUD system, but that was it. In a world where “ everything is a time-series “ none of them had stopped to wonder about the yawning gap between what they had been taught and what they were about to be dealing with. Only after falling into that gap and learning the hard way might a candidate become even potentially useful.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;How, you might ask, is an invoice a time-series ? The relationship between a supplier and customer is a time-series of documents, some of them invoices, obviously. But invoices are sent out with errors and have to be corrected, they get cancelled, they get re-negotiated - and that history must be preserved somehow, for the auditors, lawyers, regulators or whoever. It can either be managed as repeated exceptions through a CRUD system, or be managed smoothly through a system that understands that&lt;/p&gt;

&lt;p&gt;” EVERYTHING IS BI-TEMPORAL “&lt;/p&gt;</content><author><name></name></author><summary type="html">The year : 1987 The location : a Strangelove-ian conference room ( although with lots of BIG windows ) overlooking Sixth Avenue The players : me and the Great Man. He is beyond upset : veins are throbbing in both temples and, instead of just being red, his face is blotched with white. The takeaway : ( paraphrased for those of a delicate nature ) Everything is a Time-series The time-series schema he was thinking of went something like : A discrete time series is a set of time-ordered data {Xt1, Xt2, …,Xtn} obtained from observations of some phenomenon over time. Throughout this documentation we will assume, as is commonly done, that the observations are made at equally spaced time intervals. This assumption enables us to use the interval between two successive observations as the unit of time and, without any loss of generality, we will denote the time series by {X1, X2,…Xn}. Which, depressingly, was taken from a current document, rather than some well-worn, quarter-century old, yellowed papers at the back of one of my desk drawers. Why “ depressingly “ ? From www.bea.gov - discussing GDP figures for the US economy, surely one of the most widely used of all publicly-available data values : The Bureau emphasized that the first-quarter advance estimate released today is based on source data that are incomplete or subject to further revision by the source agency (see the box on page 3 and “Comparisons of Revisions to GDP” on page 5). The “second” estimate for the first quarter, based on more complete data, will be released on May 29, 2015. And as a special treat : The annual revision of the national income and product accounts will be released along with the “advance” estimate of GDP for the second quarter of 2015 on July 30. In addition to the regular revision of the estimates for the most recent 3 years and for the first quarter of 2015, some series will be revised back further (for more information, see the Technical Note). So in this case, not only will the initially published value for 1Q2015 be changed using the current methodology, but it, along with twelve other values, will shortly be recalculated using a new methodology. How would your database track these changing values for 1Q2015 ? As exceptions - when each revision as it comes in provokes a fire drill ? Or as a time-series ? - since the Xt values are already a time-series, it would need a second time dimension, but that would violate the naïve definition of a time-series and lead us into the land of bi-temporal data. In that quarter-century, relational databases have become the de facto standard technology for storing data of all kinds - because the underlying logic is so very flexible that they can, in principle, store and retrieve anything. Unfortunately, the original implementations were very crude ( Sitting on one of my bookshelves is a 113 page book from 1978, that includes the complete source code - in RT11 Basic - for a relational database system ) . And later versions went through the usual hype-cycle driven changes rather than addressing real-world problems. As a result, the number of shops where data with intrinsic ordering ( like our naïve time-series definition ) can be managed natively, is small. There were brave attempts to “ bolt-on “ time-series handling to mainstream databases in the 1990s, but in my career, I never met one in production use. Guess how much rarer a database that can handle two time dimensions natively going to be ? Most depressing of all, in the job interviews I took part in over that time, no CS graduate had the faintest idea about how to tackle these problems. Sure, they knew how to build a CRUD system, but that was it. In a world where “ everything is a time-series “ none of them had stopped to wonder about the yawning gap between what they had been taught and what they were about to be dealing with. Only after falling into that gap and learning the hard way might a candidate become even potentially useful. How, you might ask, is an invoice a time-series ? The relationship between a supplier and customer is a time-series of documents, some of them invoices, obviously. But invoices are sent out with errors and have to be corrected, they get cancelled, they get re-negotiated - and that history must be preserved somehow, for the auditors, lawyers, regulators or whoever. It can either be managed as repeated exceptions through a CRUD system, or be managed smoothly through a system that understands that ” EVERYTHING IS BI-TEMPORAL “</summary></entry><entry><title type="html">It’s 11pm, Do You Know Where Your Metadata Are ?</title><link href="/metadata/2018/01/10/its-11pm-do-you-know-where-your-metadata-are.html" rel="alternate" type="text/html" title="It's 11pm, Do You Know Where Your Metadata Are ?" /><published>2018-01-10T00:00:00-05:00</published><updated>2018-01-10T00:00:00-05:00</updated><id>/metadata/2018/01/10/its-11pm-do-you-know-where-your-metadata-are</id><content type="html" xml:base="/metadata/2018/01/10/its-11pm-do-you-know-where-your-metadata-are.html">&lt;p&gt;Look at your databases. What data types do they store ? Most likely &lt;em&gt;int&lt;/em&gt;, &lt;em&gt;date-time&lt;/em&gt;, &lt;em&gt;char&lt;/em&gt; and so on.&lt;/p&gt;

&lt;p&gt;Now reread the Business Requirements those databases were built to satisfy. What data values were to be stored ? Prices, quantities, names, addresses and so on.&lt;/p&gt;

&lt;p&gt;Where is the specification of how (for example) Order Quantity should be stored as a &lt;em&gt;smallint&lt;/em&gt; ? That specification is the metadata - and just as software documentation was a serious problem for more than 50 years until the Jupyter Notebooks and their like showed up, where and how to store metadata has been and continues to be a serious problem.&lt;/p&gt;

&lt;p&gt;For a numeric value, a complete set of metadata might look something like :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Unit&lt;/li&gt;
  &lt;li&gt;Scale&lt;/li&gt;
  &lt;li&gt;Definition&lt;/li&gt;
  &lt;li&gt;Date/time range for which the stored value is valid&lt;/li&gt;
  &lt;li&gt;Function to derive the stored value from the actual values during that date/time range ( last, mean and so on)&lt;/li&gt;
  &lt;li&gt;Date/time when the current value was stored&lt;/li&gt;
  &lt;li&gt;Date/time when this value was superseded&lt;/li&gt;
  &lt;li&gt;Lineage ( where do the values normally come from and how )&lt;/li&gt;
  &lt;li&gt;Person responsible ( the data “steward” )&lt;/li&gt;
  &lt;li&gt;If this value replaces another :
    &lt;ul&gt;
      &lt;li&gt;Reason why this value supersedes the earlier one&lt;/li&gt;
      &lt;li&gt;Who proposed the new value and when&lt;/li&gt;
      &lt;li&gt;Who approved the new value and when&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For a non-numeric value, the Unit and Scale values could be skipped: but otherwise, for a database that might be audited, inspected by a regulator, or even subject to a senior management query like “ why the %#% did we sell those widgets at that price ? “ any of those metadata items might be needed at any time.&lt;/p&gt;

&lt;p&gt;So where and how do you store all of that extra information ? - after all our single integer value now comes with a “ tail “ of two integers, five lookup codes, six date/time values and a couple of long character strings. Perhaps one or more of :&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In the original specification documents. Shame on you.&lt;/li&gt;
  &lt;li&gt;In a central, update-able library ( like a wiki ). At least this way there’s a chance that the metadata will be kept up to date: the chance is higher the more aggressively your documentation policies are policed. And eventually you might even find yourselves with a company-wide data dictionary.&lt;/li&gt;
  &lt;li&gt;In the database along with the original value. There would of course be much complaining about “ speed “ and “ space “, but so long as the data value and its metadata live in the same database we have the opportunity to make sure that values and their documentation are kept in sync.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There’s no simple one-size-fits-all answer. From what I’ve seen over the years, there’s been precious little effort made to ensure that data fields and values are properly documented at all. But without that effort, there will always be surprises - like the fire drill when a Price that is expected to be expressed in USD, arrives in Roubles - and those surprises will require an immediate response, not a years-long review of data quality.&lt;/p&gt;

&lt;p&gt;Fifty years ago, the first computing acronym I learned was GIGO - “ Garbage in, garbage out “. It’s as true now as it ever was. No matter how clever your algorithm, how modern your language, how fancy your tools, if your code is given unexpected data it will give unexpected results. Writing that code is - or should be - a one-off exercise, maintaining the data it needs to work properly is a shift-by-shift, day-by-day grind that for too long has been pushed into the background by IT departments chasing after new toys.&lt;/p&gt;

&lt;p&gt;What is needed is not new data management tools - there are lots around and there will be even more to choose from in the future - but management commitment to support the never-ending effort required to deliver “ clean “ data and an understanding that the people tasked with that job are in today’s world among the most important ones in the Firm, but that they can only do that job if the data is properly documented.&lt;/p&gt;

&lt;p&gt;Two points&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Call me a pedant but &lt;em&gt;data&lt;/em&gt; is a plural noun, the singular form is &lt;em&gt;datum&lt;/em&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Anyone who watched the evening news in New York in the late 1980’s will recognize the title&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Look at your databases. What data types do they store ? Most likely int, date-time, char and so on. Now reread the Business Requirements those databases were built to satisfy. What data values were to be stored ? Prices, quantities, names, addresses and so on. Where is the specification of how (for example) Order Quantity should be stored as a smallint ? That specification is the metadata - and just as software documentation was a serious problem for more than 50 years until the Jupyter Notebooks and their like showed up, where and how to store metadata has been and continues to be a serious problem. For a numeric value, a complete set of metadata might look something like : Unit Scale Definition Date/time range for which the stored value is valid Function to derive the stored value from the actual values during that date/time range ( last, mean and so on) Date/time when the current value was stored Date/time when this value was superseded Lineage ( where do the values normally come from and how ) Person responsible ( the data “steward” ) If this value replaces another : Reason why this value supersedes the earlier one Who proposed the new value and when Who approved the new value and when For a non-numeric value, the Unit and Scale values could be skipped: but otherwise, for a database that might be audited, inspected by a regulator, or even subject to a senior management query like “ why the %#% did we sell those widgets at that price ? “ any of those metadata items might be needed at any time. So where and how do you store all of that extra information ? - after all our single integer value now comes with a “ tail “ of two integers, five lookup codes, six date/time values and a couple of long character strings. Perhaps one or more of : In the original specification documents. Shame on you. In a central, update-able library ( like a wiki ). At least this way there’s a chance that the metadata will be kept up to date: the chance is higher the more aggressively your documentation policies are policed. And eventually you might even find yourselves with a company-wide data dictionary. In the database along with the original value. There would of course be much complaining about “ speed “ and “ space “, but so long as the data value and its metadata live in the same database we have the opportunity to make sure that values and their documentation are kept in sync. There’s no simple one-size-fits-all answer. From what I’ve seen over the years, there’s been precious little effort made to ensure that data fields and values are properly documented at all. But without that effort, there will always be surprises - like the fire drill when a Price that is expected to be expressed in USD, arrives in Roubles - and those surprises will require an immediate response, not a years-long review of data quality. Fifty years ago, the first computing acronym I learned was GIGO - “ Garbage in, garbage out “. It’s as true now as it ever was. No matter how clever your algorithm, how modern your language, how fancy your tools, if your code is given unexpected data it will give unexpected results. Writing that code is - or should be - a one-off exercise, maintaining the data it needs to work properly is a shift-by-shift, day-by-day grind that for too long has been pushed into the background by IT departments chasing after new toys. What is needed is not new data management tools - there are lots around and there will be even more to choose from in the future - but management commitment to support the never-ending effort required to deliver “ clean “ data and an understanding that the people tasked with that job are in today’s world among the most important ones in the Firm, but that they can only do that job if the data is properly documented. Two points Call me a pedant but data is a plural noun, the singular form is datum Anyone who watched the evening news in New York in the late 1980’s will recognize the title</summary></entry></feed>